{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LAyUYsqf9yi",
        "outputId": "bcbddf90-6d57-4b2c-c051-133085c7d799"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall -y torch torchvision torchaudio\n",
        "# !pip install --upgrade --no-cache-dir --no-deps unsloth\n",
        "# !pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0\n",
        "# !MAX_JOBS=4 pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mrWXRT68XV1",
        "outputId": "b7a97bc6-4d6b-4cef-b1c1-22bdc027b490"
      },
      "outputs": [],
      "source": [
        "# !pip install -U bitsandbytes transformers peft accelerate trl datasets sentencepiece wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWhDZEGgiFj6",
        "outputId": "b574c18f-24b6-47b5-867e-2914c6913bc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrT7idMPja5L"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-IHDHaKj8CT"
      },
      "outputs": [],
      "source": [
        "USE_GPU = True\n",
        "if USE_GPU:\n",
        "  device = \"cuda:0\"\n",
        "else:\n",
        "  device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxoYXDVWkYp0"
      },
      "outputs": [],
      "source": [
        "QUANTIZE_4BIT = True\n",
        "USE_GRAD_CHECKPOINTS = True\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "TRAIN_MAX_SEQ_LENGTH = 512\n",
        "USE_FLASH_ATTENTION = True\n",
        "GRAD_CC_STEPS = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmrs9wy7kti6"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"CohereForAI/aya-101\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZgRL3C4lNIo"
      },
      "outputs": [],
      "source": [
        "quantization_config = None\n",
        "if QUANTIZE_4BIT:\n",
        "  quantization_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_quant_type=\"nf4\",\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc9DRBDwlzFI"
      },
      "outputs": [],
      "source": [
        "attn_implentation = None\n",
        "if USE_FLASH_ATTENTION:\n",
        "  attn_implementation=\"flash_attention_2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901,
          "referenced_widgets": [
            "7616ea8ba536427da92ecb34eceb4af9",
            "868239ce71ac46069cd11e5f5a257e1a",
            "402e908dc11947dab28f0b92317851c3",
            "d19a0465ef9d4e8589849b6a9771592c",
            "da38a526b3da4e29bf7ef90684f6885c",
            "3eec3cf8b4644c64ac23654a5f9de35f",
            "12b1d57ad3c04762b74ebaaf4c8e18e1",
            "4e5cc26c52574a7b912d2964dc527df2",
            "4efa6c4c780d41e38246633fff391f12",
            "7654359d7912418b89e3bf9649a3c8da",
            "2d8044480b7e4f29b005d3e2fb202ec1",
            "b7c4f5c763b847ba8e393d61ac8c2fd2",
            "0ac75bf06460473fb443ded6fb754bb3",
            "848f64b8c9b549c59ab2a41cd76a9e7c",
            "e74bf0a66d244cfcbb18a77e5c711b91",
            "4e6f017c93464461adb9eff5a252e2e8",
            "a8841f2aaa7148b5bdd7aa131267fcc8",
            "e35e9b7888bd4b38af02a183347e75fc",
            "17251683b4174451829e1f112ef4ef02",
            "cff13c4034c64bf6b724057fccb5c0d4",
            "fd8cd4e675fb44039ae1fa7acb570b6b",
            "b91e3a0c6f8f4bcb942bdaea38c26af9",
            "fe9edb62794e4f689fa22a0247e66037",
            "fc8a848730d04627b667bbb5a2b4600f",
            "51599d140318434193eb5d4b306c1511",
            "b8f68dc8e3c2451890a7ee7735511268",
            "5164dffb349a4f95a326d50b43ecb265",
            "3625d9eca70e42af8dd27a6b40bab147",
            "9e6c2c3bad224791a8503d1a98bc5c2c",
            "fb6adf33126e40e5ab277350841a8cc3",
            "b0812e8708cc4ef1bf761db4445fd441",
            "aad88feb1bb347f08d9835d9cb43a02c",
            "49f8f24ba7e541f38c90417bdccbd1b6",
            "b3cb8f6e93c94cc88b4383d5fead25fe",
            "7ab53f8c92544a21899683cc2c2da9dc",
            "a49f85fcf97c4fd9b62a60b2c304eb59",
            "7ec0dc23c7b64d4ab29947510318190f",
            "cc43bc0ec7644ee68a89ee9770bbb59c",
            "fd3320076fb84802bac4b8f038951a62",
            "906ab31dff3e4d68a46f25e6f6fd5b0b",
            "60c564e2c4fe416ea03bca8d59785833",
            "681b27c70c704027841f52018d5bc5f9",
            "ca5c6200eb7147ada1f40df3a4ef3b0c",
            "f3ebdf88207e405d9ea5026217c45a29",
            "0beb076d42ed499397fce2c1c7a06582",
            "ea21c6ee70eb414db0d8ea1911b23fde",
            "7f986b55c0344d47964defdc91eabe38",
            "e5aa1364654544b9acb2226001cada22",
            "13e91fdc90594c839eb62fa55f25f419",
            "64398b137eeb4594a97e0cdd2b8229f6",
            "d916f306f6a94e479caf964850298373",
            "62d4b883557346b69454ad3c08a3763f",
            "f6ecd3c21fbe4f3494defd9488d0a478",
            "4043f8936b37488bb041b63678963c72",
            "e27b1ac321d14518b3931810eb390362",
            "d52b1980d298450c866d3237dcdd680b",
            "6e1e7d99c81f415eb3346a5bbd8a6e27",
            "ff23b94836d64a0caca89ef802c6ae33",
            "cbc3c99ec63240c3951ee1d13388f65b",
            "3ef3ae48e80f41eaab8445d4a6391159",
            "570e6433612344ae986be269ca84c509",
            "b7223a9801534da2bb8052d396242585",
            "723a4f61130141c7b0b35f8109d15bac",
            "941f68b920db43e0837f6d6f32cdc7df",
            "d58fdd4697ed43afb6bb54984b223625",
            "3ea527c8e7b8477d93788626cfd10fe0",
            "1277300a752c4c5bb9802f33c1e6514f",
            "5974fda9b5814901984de290b76172a2",
            "21b3c078f8e148c18b4e1b7c0e5dd7da",
            "5dbbe977458548ed91bae614884df0ef",
            "058a8b5e8990431d932278762bfc23e8",
            "be24d88bbb7e49bf8646cdd0fd7b3d52",
            "d841182f2f574ee3b3035ea7ddcb7d48",
            "823f0edd2b1146f3828a69a5f9c61b6e",
            "b4f0750e6b304487a0b3f136fcb7b1ad",
            "25a63ffe9ad04e61ae795a0b30666af9",
            "c5b6081acf784ae1a1664b1a8ef0e04f",
            "26f83a7830c94b5f93dfeceb672da458",
            "ae1ff229640b4d8ab5050d9bdbcae752",
            "85f9def785c14ecd978570bd6cb1f59a",
            "0683714976df4f84bef32f53d10aae76",
            "979aa31c77ab460da3879b89be6963b8",
            "feb4facff0b640f7a5ddc99ad5980d29",
            "3f3b75505f9e45fdbf3e41045608a631",
            "b2dd97a828a1410babdf3d67fedebc2c",
            "db81d266d03f4b9f9b42ed8fdb6c9c17",
            "ac32ffa85445462383cda7d6ae72abf2",
            "41c5835dbbdc42919b021481160b0b6b",
            "e02e14a9a01a4cd6ac4338cc326c4099",
            "10ed37005bb8423ab0f98d29ec308931",
            "1fdf0aa084f74db7b91c6c46292f2c28",
            "a48721f893ec4018acaec035bdce5a1e",
            "f1304f3134454670afceac9b9072d2cd",
            "57777d67e00f42798db33b342de65086",
            "a5e40f329ca4480ab0b4fb2514ea2f0f",
            "757de7d23f3d4b43a53d8dfa77fa3c7f",
            "6849a70935b8460f951768a5a7dade21",
            "19e6d0824d764fd6a369611c06aa6684",
            "15a1f4362d6a4ae09f60decfab068df1",
            "e4b1a375960141f0bfebac79ced5c8af",
            "cf2b878995a047c99f02a0651ce564e5",
            "fc89b9fcb88b46a69bfa8980e9f56596",
            "d655fdf7512048bfb832541800d56645",
            "c13942efc6604208abffb2988c071b89",
            "120d61796e424251898693e4e13a9ed5",
            "ac48e549e99b4801b8896f0a2ddbf4ae",
            "9d30cb895e6344c98737c683551b38cd",
            "423e3b2ada5d42b7854e1af8d509af1a",
            "e9dbfe110ff248c386e1b4efb58edb42",
            "f448bcb94440490aa126066bec964e77",
            "7f370d5217f24e13a7155718692a7d72",
            "399fdd2efdc6457e8250aa6538468dc3",
            "f7164617206740fba4c7572f95627afa",
            "fafbe2fb4a8e437ea6f09d4ff129003b",
            "8cb712fa7e744b778bdc4924c15aacd4",
            "c3623802759643dbbd488f357cdc0d1f",
            "753289fb6c6b42cd8ba665bfda0977b6",
            "7688c927a8e74c848f4d7e1362e28e37",
            "b1bb7c7e4fe0498db48f679d6c3c1ca5",
            "c0290ea6ad614096ae82e655addc7a1d",
            "a7fa0ec8be2c4f43b4901617ab8faea6",
            "658e5f782b1d4bff8032d41e5cf9b930",
            "aaca9df0ecc8413e89149ff1f5ae1b3d",
            "17d1147e222c462c9af34f7e99f27763",
            "8f735093069c4947a1685310cc5bb2ac",
            "e40b00410c4c4c2ba5c763528ed0886e",
            "48a43ae054ee421880530fbdc4a00cad",
            "65b184db344b4b0fbb54c2e4c1cb41b5",
            "0b83b0937d9e47d58cf5c65824e32490",
            "9f3ab321c90846918cd7e44399604240",
            "ba00f2192f4e4720b42113689ae3bf32",
            "5d94f688710948c5a27cb11e60a359d6",
            "065db106e1b34e8f924779d3074798ff",
            "f82ca64cee964649b9a0d4d56feb7eb0",
            "b7f86a77eb1e4a148d128617121a2873",
            "69bb9eebbc2145d0a0512ded45585136",
            "a3fac916ed5c4ad8953f56dbb783345a",
            "d4b2a51b6cbe4e358845f5753ac598c1",
            "7d57c568dbb947edb8d5af43f8f0a6f6",
            "e05534d7c51d4ec7a7ff71afe114ad9e",
            "3ee99221e085404f8939299fadebac35",
            "bc738d138ced444db149e1d099a65d57",
            "62022f840df248348b75d9940ffaa5e8",
            "8568cd536d804a928546aaaa72767bc4",
            "18ba4a9e12f44ba49c2a6c8c3162e615",
            "397c18dcb5ba43dfac0ee0eabb8b0521",
            "55ac9e3b51784035b01119dc1f0b1bd8",
            "7233f4714aeb48a19b0b11afb7b20f24",
            "7735f7a1e0b74694b58845f445f03129",
            "db470165b0214999a1f5db6dcd4a6c73",
            "7f03b316ac924becb3e7f1bad8e7c748",
            "e6d0d50e495e4510beaaefe89a02e6b0",
            "9265963204a440918bb3340d395f4de3",
            "6cc80977955e4d9087797406ba244a2e",
            "fb0466cae0ac45f594d5d7f82ae519c4",
            "d872c3d522a24d9488648863bbcb8db2",
            "a4033aa41a324c399cde1bd033fa4105",
            "5b7f425aae9c44abb83ed51700857c59",
            "01ed9183c79f46fbb54d589ef5b6a1f6",
            "a04e5cfb6e6f4e7ba6ab5b6dc5f0b2ff",
            "f89d6fbb532545bda7647081e472838f",
            "bb75e97d4e114ab5b8d40822aa43f285",
            "d42b123cff9c41cb8c09eed7268851e8",
            "6d8912ab94bf4a4ca20df2620b14e5d7",
            "4fed6ce7b20b4fa083ff9b43828edc32",
            "3031b4a7922b4d6090d8db7d988139d2",
            "858185d3f739454e95163d9cddff4e92",
            "7c7dc6ed4390401ab155e95a44aa7fdf",
            "a6643671b7404e1e9d6215d810efcfe5",
            "c62a3259f27e46ff99e99cd3afe4022b",
            "a7abc8314b5448e3808891ec48c4439a",
            "44ac29d58c1340859bde9777a9d23025",
            "8671cb7ae9894499bdadac1a9a5a7647",
            "5a33e20488fc4500b336b71386429cdc",
            "b276b2b379d74ab6a0ced09a7cd722cf",
            "0afa7a5446444942a7bb759d6c87dbaf",
            "5639d34feb08430a9d7014dbda18580d",
            "7026b95d0d034637a184b4cf3c52e0cc",
            "603afe7f205740f680b66616545cde6e",
            "9f131c0958014a59b6f8dd526e3d4381",
            "373d270e3adf4ad8aaaba3d77906cbcb",
            "48b64f60bdc44ca1a7ee8da52bb3d756",
            "e1fbf6b65b3548359c40e405d5c935b9",
            "bb3a5c9f79a04a7aa96414546738cfa6",
            "4617ee5983084f8abd816fa47daadd9a",
            "0f9e3c342bfe420fab420bd578da5ace",
            "676ea0ba7e0e4d5dab5c03213a7b75c0",
            "199ae21b8b4640f9bf1028fb1de57d1a",
            "f6d4df84d9954a2aacee3b873f3560a9",
            "1e70097cfd8849579fa1f37f558c3c87",
            "9a3ca61adda84118b75708f533ccf840",
            "eebaabace28e40c783c7a3ec1fad95ad",
            "eb92290e11614fccbfc6f3145a838f99",
            "96ef34cf9e6b49cb86465630baadc413",
            "764a5807c7e8477bb6496e53123c2b38",
            "dfb7a2a81389477ba06583eaa293cdb8",
            "2da285548350495c831c85a7d93d16c6",
            "f704c36f6b6d4b41b4ac0e8c7299afe2"
          ]
        },
        "id": "yTxwfjFvmDfj",
        "outputId": "e011b629-acc5-4acb-dcf0-b399e93235fb"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=quantization_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD9z8NmAzoyC"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset_dict = load_dataset(\"csv\", data_files=\"/content/train_parallel_for_llm_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chMV2P0ZhYaF"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset_dict[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFpYnbEVhel9",
        "outputId": "b227a3f0-a5e4-41fc-aafb-f3f5aabe3444"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['inputs', 'targets'],\n",
              "    num_rows: 2091\n",
              "})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07nn2DCc5BCD"
      },
      "outputs": [],
      "source": [
        "MAX_SOURCE_LENGTH = 512\n",
        "MAX_TARGET_LENGTH = 512\n",
        "TRAIN_MAX_SEQ_LENGTH = 512\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"inputs\"], max_length=MAX_SOURCE_LENGTH, padding=False, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"targets\"], max_length=MAX_TARGET_LENGTH, padding=False, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOhfw6qXt53a"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from transformers import DataCollatorForSeq2Seq, DataCollatorWithPadding\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "GRAD_ACC_STEPS = 1\n",
        "USE_GRAD_CHECKPOINTING = False\n",
        "MAX_SEQ_LENGTH = 512\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"results\",\n",
        "    num_train_epochs=15,\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=2,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch\",\n",
        "    save_steps=400,\n",
        "    logging_steps=50,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    warmup_ratio=0.05,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"none\",\n",
        "    label_names=[\"labels\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0NJo6yWwKcd"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=32,\n",
        "    r=32,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\",\n",
        "    target_modules=[\"q\", \"v\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWhS_mphxYbT"
      },
      "outputs": [],
      "source": [
        "tokenizer.model_max_length = MAX_SEQ_LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "59168747d7a04926b597fbf1a4db02ad",
            "879065162df9468ebcf1c80c2bc8dfca",
            "bc70e24f3fd34f3d806fefcfc03480d8",
            "01b5b2226c2d4d9fbb4ab9ff1c8c81a9",
            "56df1b2fd54346b2b465c5038d80a95d",
            "0b74c6c5a8b84d2ca2e65efc2b0454dd",
            "31fab1da82aa464eba1f8b22c42558ab",
            "0aa0cb96582745729f97ef2fa7be304f",
            "78ec9fff2b6e4d5dab888839d93122e1",
            "f27ce9c11d344f969292308473a20922",
            "78a6a069bcbd445095afc31eeada78ba"
          ]
        },
        "id": "miAUfGN9wmX3",
        "outputId": "a0849362-a4d8-4fcf-ab6a-e70530a8780e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59168747d7a04926b597fbf1a4db02ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/2091 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    peft_config=peft_config,\n",
        "    data_collator=data_collator,\n",
        "    args=training_arguments,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k10-C2OvqavJ",
        "outputId": "ffb009d3-6d67-47ec-d9f4-84f8ca6edb41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer length before resizing: 250100\n",
            "Tokenizer length after resizing: 250100\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tokenizer length before resizing: {len(tokenizer)}\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "print(f\"Tokenizer length after resizing: {len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A5fXTMe-4apT",
        "outputId": "1ceca179-f411-427b-c955-47374330e2b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3915' max='3915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3915/3915 1:41:09, Epoch 14/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.566100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.341200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.327800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.252800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.279000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.222500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.251300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.229700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.226600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.238600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.217000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.214400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.221700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.219500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.173200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.216000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.170200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.181300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.182900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.176100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.183600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.146000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.150600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.181000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.157800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.158000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.138400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.152800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.145900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.119100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.135700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.143400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.148200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.127900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.102500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.122300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.124400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.109100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.119700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.114000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.113300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.093300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.095400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.099300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.111600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.110500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.110200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.099500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.080100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.096800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.103800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.102500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.100400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.085500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.073200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.108000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.083700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.085100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>0.072100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.073300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>0.092100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.081500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>0.080600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.072100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.081000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.078500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>0.076800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.077400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>0.068500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.079000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3550</td>\n",
              "      <td>0.071800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.070300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3650</td>\n",
              "      <td>0.089300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.068600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>0.060100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.074200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3850</td>\n",
              "      <td>0.071300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.078500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(250100, 4096)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(250100, 4096)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 64)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear4bit(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear4bit(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-23): 23 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear4bit(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear4bit(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(250100, 4096)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 64)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear4bit(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear4bit(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-23): 23 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (v): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear4bit(in_features=4096, out_features=10240, bias=False)\n",
              "              (wi_1): Linear4bit(in_features=4096, out_features=10240, bias=False)\n",
              "              (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=250100, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()\n",
        "\n",
        "trainer.model.save_pretrained(save_directory='/content/aya-101_training')\n",
        "\n",
        "model.config.use_cakthe = True\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "058fd212e8184288b417675d74159e26",
            "48eaffeff8874f0e8701a3cf1b406c3e",
            "1ebc7a4208be4a72b58a9dbc94a1f523",
            "eb472653877242a99ee973577cc3e724",
            "06e6bd972995486daa171f8afa8e117b",
            "299d3c87e85c447980b49324a8f19219",
            "e8fda15b1df84df19c89ad6647112cf8",
            "62661b44589e4076b114c8c294f2c66d",
            "79b23fbf211642e9864e2913c6a29353",
            "0c37ed5f4dca4f7ea4c0d0ee30a939a5",
            "ef2c55d2f1bd4ef1b6728e3f93746b8c"
          ]
        },
        "id": "41rVVCYz05dX",
        "outputId": "cc511abf-f737-4fba-e12d-36c347276488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adapter saved locally to /content/aya-101_training\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "058fd212e8184288b417675d74159e26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/6.30G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adapter pushed to Hugging Face Hub at linndfors/uk-sent-gender-swapper_aya-101\n"
          ]
        }
      ],
      "source": [
        "hf_repo_name = \"linndfors/uk-sent-gender-swapper_aya-101\"\n",
        "\n",
        "local_save_path = '/content/aya-101_training'\n",
        "trainer.model.save_pretrained(local_save_path)\n",
        "print(f\"Adapter saved locally to {local_save_path}\")\n",
        "\n",
        "trainer.model.push_to_hub(hf_repo_name)\n",
        "print(f\"Adapter pushed to Hugging Face Hub at {hf_repo_name}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "new_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
