{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linndfors/study/diploma/new_venv/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "import stanza\n",
    "import os\n",
    "import numpy as np\n",
    "import spacy\n",
    "import tqdm\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for the agreement accuracy (pymorhy lib to check gender to avoid “вона робив”) stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Ukrainian model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e0f2b728db4dc6914dbaea201fcba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 23:32:55 INFO: Downloaded file to /Users/linndfors/stanza_resources/resources.json\n",
      "2025-03-04 23:32:55 INFO: Downloading default packages for language: uk (Ukrainian) ...\n",
      "2025-03-04 23:32:57 INFO: File exists: /Users/linndfors/stanza_resources/uk/default.zip\n",
      "2025-03-04 23:33:02 INFO: Finished downloading models and saved to /Users/linndfors/stanza_resources\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading Ukrainian model...\")\n",
    "stanza.download('uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_nlp = stanza.Pipeline('uk', verbose=False, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gender_agreement_in_sentences(sentences):\n",
    "    results = {}\n",
    "    for sentence in tqdm.tqdm(sentences):\n",
    "        doc = uk_nlp(sentence)\n",
    "        \n",
    "\n",
    "        for sent in doc.sentences:\n",
    "            for word in sent.words:\n",
    "                try:\n",
    "                    if word.deprel in [\"nsubj\", \"nsubj:pass\"] and \"Gender\" in str(word.feats):\n",
    "                        if \"Animacy=Inan\" not in str(word.feats):\n",
    "                            subj_text = word.text\n",
    "                            subj_feats = word.feats\n",
    "                            subj_gender = [feat.split('=')[1] for feat in subj_feats.split('|') if feat.startswith('Gender')][0]\n",
    "                            subj_number = [feat.split('=')[1] for feat in subj_feats.split('|') if feat.startswith('Number')][0]\n",
    "\n",
    "                            head_id = word.head\n",
    "                            if head_id > 0:\n",
    "                                verb = sent.words[head_id - 1]\n",
    "                                if verb.pos == \"VERB\" and \"Gender\" in str(verb.feats):\n",
    "                                    verb_text = verb.text\n",
    "                                    verb_feats = verb.feats\n",
    "                                    verb_gender = [feat.split('=')[1] for feat in verb_feats.split('|') if feat.startswith('Gender')][0]\n",
    "                                    verb_number = [feat.split('=')[1] for feat in verb_feats.split('|') if feat.startswith('Number')][0]\n",
    "\n",
    "                                    if subj_gender == verb_gender and subj_number == verb_number:\n",
    "                                        pass\n",
    "                                        # print(f\"AGREE: {subj_text} ({subj_feats}) і {verb_text} ({verb_feats})\")\n",
    "                                    else:\n",
    "                                        results[sentence] = (subj_text, verb_text)\n",
    "                                        # print(\"\\nSentence:\", sentence)\n",
    "                                        # print(f\"XXX NOT AGREE XXX: {subj_text} ({subj_feats}) і {verb_text} ({verb_feats})\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error {e}; in the following sentence: {sentence}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/518 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 31/518 [00:47<19:03,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error list index out of range; in the following sentence: В наших умовах у таких «рольових коконах» продовжували дефілювати партійні ортодоксики, лектора ЦК Компартії, які своєю поведінкою дуже нагадували героїню Кіри Сазерленд (чи то героїня «Меланхолії» через двадцять п’ять років наслідує їх?..) , та ще офіцерки КҐБ, які єдині тоді мусили бути щасливі, бо їм додалося праці: в кінці травня одна така приходила вербувати й мене (цей сюжет я потім використала для схожого епізоду в біографії Віктора в «Музеї покинутих секретів»), і ми з нею близько трьох годин мужньо блукали вулицями «на свіжому повітрі», дивуючи нечастих перехожих (вона, правда, запропонувала піти до ресторану – на той час гуляти «на свіжому повітрі» вже не рекомендувалося ц\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 289/518 [05:11<05:30,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error list index out of range; in the following sentence: Обидві Колеснікови згадувались як членкині наглядової ради ЗАТ «Енергомережа» до того, як фірму очолила Ольга Крючкова, що останні роки представляє інтереси нардепки від БПП Ігоря Кононенка.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518/518 [08:57<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "results_of_agreement_test = check_gender_agreement_in_sentences(list(test_gender_swapped_df['swapped']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_of_agreement_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [19:01<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "results_of_agreement_dev = check_gender_agreement_in_sentences(list(dev_gender_swapped_df['swapped']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_of_agreement_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agreement mistake for dev set: 0.0439\n"
     ]
    }
   ],
   "source": [
    "print('agreement mistake for dev set:', \"{:.4f}\".format((len(results_of_agreement_dev) / len(dev_gender_swapped_df['swapped']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agreement mistake for test set: 0.0367\n"
     ]
    }
   ],
   "source": [
    "print('agreement mistake for test set:', \"{:.4f}\".format((len(results_of_agreement_test) / len(test_gender_swapped_df['swapped']))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/linndfors/study/diploma/uk-gender-word-mapper/common_gender_words_list.txt', 'r', encoding='utf-8') as file:\n",
    "    common_gender_words_list = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 out of 19 agreement mistakes are common gender words\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for k, v in results_of_agreement_test.items():\n",
    "    v1, v2 = v\n",
    "    if v1.lower() in common_gender_words_list or v2.lower() in common_gender_words_list:\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "print(f'{counter} out of {len(results_of_agreement_test)} agreement mistakes are common gender words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate annotation project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_swapped = pd.read_csv(\"/Users/linndfors/study/diploma/ner_for_fem/data/utils_files/test_swapped.csv\", index_col=0)\n",
    "dev_swapped = pd.read_csv(\"/Users/linndfors/study/diploma/ner_for_fem/data/utils_files/dev_swapped.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_annotation = pd.concat([dev_swapped, test_swapped]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    1473\n",
       "True       69\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_after_annotation['Оригінальне речення'] == df_after_annotation['Змінене речення']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Помічник/ця\n",
       "Сніжанна Б              23\n",
       "Олеся Д.                19\n",
       "Тарас Я.                17\n",
       "Оля Н.                  11\n",
       "Сніжанна Б.             10\n",
       "Андрій Рудницький        9\n",
       "Анастасія Г              8\n",
       "Крищук Катерина          7\n",
       "Анна К.                  7\n",
       "Вікторія Мокряк          6\n",
       "Юлія М.                  6\n",
       "Таміла К.                4\n",
       "Королевська Ольга        4\n",
       "Олександр К.             3\n",
       "Дуда Луїза               3\n",
       "Анастасія Г\\n            3\n",
       "Оля Н                    3\n",
       "Ольга Ш.                 2\n",
       "Королевська О.           2\n",
       "Петро І.                 2\n",
       "Королевська О.Д          1\n",
       "Андрій Р.                1\n",
       " Сніжанна Б.             1\n",
       "Мар'яна                  1\n",
       "Юлія М                   1\n",
       "Олеся Д,                 1\n",
       "\\n Андрій Рудницький     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_after_annotation['Помічник/ця'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Оригінальне речення</th>\n",
       "      <th>Змінене речення</th>\n",
       "      <th>Анотації</th>\n",
       "      <th>Коректність речення</th>\n",
       "      <th>Виправлене речення</th>\n",
       "      <th>Помічник/ця</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Виставу за п'єсою російського класика Льва Тол...</td>\n",
       "      <td>Виставу за п'єсою російського класика Льва Тол...</td>\n",
       "      <td>{'Льва Толстого': 'PERS', 'режисера': 'JOB', '...</td>\n",
       "      <td>Містить помилки</td>\n",
       "      <td>Виставу за п'єсою російської класикині Марії Т...</td>\n",
       "      <td>Оля Н</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Віртуозні Лесь Задніпровський – князь Абрезков...</td>\n",
       "      <td>Віртуозні Ганна Задніпровська – княгиня Абрезк...</td>\n",
       "      <td>{'Лесь Задніпровський': 'PERS', 'князь': 'JOB'...</td>\n",
       "      <td>Містить помилки</td>\n",
       "      <td>Віртуозні Ганна Задніпровська – княгиня Абрезк...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Старша в Києві , одружена з військовим , чолов...</td>\n",
       "      <td>Старша в Києві , одружена з військовою , чолов...</td>\n",
       "      <td>{'військовим': 'JOB'}</td>\n",
       "      <td>Містить помилки</td>\n",
       "      <td>Старша в Києві , одружена з військовою , жінка...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ні , не так - автор статті , позаштатний корес...</td>\n",
       "      <td>Ні , не так - авторка статті , позаштатна коре...</td>\n",
       "      <td>{'позаштатний кореспондент': 'JOB'}</td>\n",
       "      <td>Правильне</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>І лектори в сірих піджаках із червоними парткв...</td>\n",
       "      <td>І лектори в сірих піджаках із червоними парткв...</td>\n",
       "      <td>{'священиків': 'JOB'}</td>\n",
       "      <td>Правильне</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Оригінальне речення  \\\n",
       "0  Виставу за п'єсою російського класика Льва Тол...   \n",
       "1  Віртуозні Лесь Задніпровський – князь Абрезков...   \n",
       "2  Старша в Києві , одружена з військовим , чолов...   \n",
       "3  Ні , не так - автор статті , позаштатний корес...   \n",
       "4  І лектори в сірих піджаках із червоними парткв...   \n",
       "\n",
       "                                     Змінене речення  \\\n",
       "0  Виставу за п'єсою російського класика Льва Тол...   \n",
       "1  Віртуозні Ганна Задніпровська – княгиня Абрезк...   \n",
       "2  Старша в Києві , одружена з військовою , чолов...   \n",
       "3  Ні , не так - авторка статті , позаштатна коре...   \n",
       "4  І лектори в сірих піджаках із червоними парткв...   \n",
       "\n",
       "                                            Анотації Коректність речення  \\\n",
       "0  {'Льва Толстого': 'PERS', 'режисера': 'JOB', '...     Містить помилки   \n",
       "1  {'Лесь Задніпровський': 'PERS', 'князь': 'JOB'...     Містить помилки   \n",
       "2                              {'військовим': 'JOB'}     Містить помилки   \n",
       "3                {'позаштатний кореспондент': 'JOB'}           Правильне   \n",
       "4                              {'священиків': 'JOB'}           Правильне   \n",
       "\n",
       "                                  Виправлене речення Помічник/ця  \n",
       "0  Виставу за п'єсою російської класикині Марії Т...       Оля Н  \n",
       "1  Віртуозні Ганна Задніпровська – княгиня Абрезк...         NaN  \n",
       "2  Старша в Києві , одружена з військовою , жінка...         NaN  \n",
       "3                                                NaN         NaN  \n",
       "4                                                NaN         NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_after_annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset: 1542\n",
      "\n",
      "Correct sentence: 58.4 %\n",
      "Uncorrect sentence: 37.5%\n",
      "Difficult to determine: 3.8%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of the dataset: {len(df_after_annotation)}\\n\")\n",
    "\n",
    "print(f\"Correct sentence: {len(df_after_annotation[df_after_annotation['Коректність речення']=='Правильне']) * 100 / len(df_after_annotation):.1f} %\")\n",
    "print(f\"Uncorrect sentence: {len(df_after_annotation[df_after_annotation['Коректність речення']=='Містить помилки']) * 100 / len(df_after_annotation):.1f}%\")\n",
    "print(f\"Difficult to determine: {len(df_after_annotation[df_after_annotation['Коректність речення']=='Важко визначити']) * 100 / len(df_after_annotation):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens count match for Original, GPT-generated and Corrected/Annotated Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linndfors/study/diploma/new_venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/linndfors/study/diploma/new_venv/lib/python3.10/site-packages/spacy_transformers/layers/hf_shim.py:137: UserWarning: Error loading saved torch state_dict with strict=True, likely due to differences between 'transformers' versions. Attempting to load with strict=False as a fallback...\n",
      "\n",
      "If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current 'transformers' and 'spacy-transformers' versions. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "ner_nlp = spacy.load(\"uk_ner_web_trf_13class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1542it [19:22,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "orig_vs_gpt_match = 0\n",
    "gpt_vs_ann_match = 0\n",
    "orig_vs_false_gpt = 0\n",
    "orig_vs_ann_match = 0\n",
    "total_with_ann = 0\n",
    "total = 0\n",
    "\n",
    "for x, row in tqdm.tqdm(df_after_annotation.iterrows()):\n",
    "    ann_swapped_sent = None\n",
    "    orig_sent = ner_nlp(row['Оригінальне речення'])\n",
    "    gpt_swapped_sent = ner_nlp(row['Змінене речення'])\n",
    "\n",
    "    if len(orig_sent) == len(gpt_swapped_sent):\n",
    "        orig_vs_gpt_match += 1\n",
    "        if row['Коректність речення'] == \"Містить помилки\":\n",
    "            orig_vs_false_gpt += 1\n",
    "\n",
    "    if row['Коректність речення'] == \"Містить помилки\":\n",
    "        ann_swapped_sent = ner_nlp(row['Виправлене речення'])\n",
    "        total_with_ann += 1\n",
    "\n",
    "        if len(gpt_swapped_sent) == len(ann_swapped_sent):\n",
    "            gpt_vs_ann_match += 1\n",
    "\n",
    "        if len(orig_sent) == len(ann_swapped_sent):\n",
    "            orig_vs_ann_match += 1\n",
    "\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 1542\n",
      "Total with annotated correction: 579\n",
      "\n",
      "Metric 1 - Orig vs GPT token count match: 1496 (97.02%)\n",
      "Metric 2 - GPT vs Annotated token count match: 557 (96.20%)\n",
      "Metric 3 - Orig vs Annotated token count match: 550 (94.99%)\n",
      "Metric 4 - Orig vs wrong GPT token count match: 550 (94.99%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total sentences:\", total)\n",
    "print(\"Total with annotated correction:\", total_with_ann)\n",
    "print(\"\\nMetric 1 - Orig vs GPT token count match:\", orig_vs_gpt_match, f\"({orig_vs_gpt_match / total:.2%})\")\n",
    "print(\"Metric 2 - GPT vs Annotated token count match:\", gpt_vs_ann_match, f\"({gpt_vs_ann_match / total_with_ann:.2%})\")\n",
    "print(\"Metric 3 - Orig vs Annotated token count match:\", orig_vs_ann_match, f\"({orig_vs_ann_match / total_with_ann:.2%})\")\n",
    "print(\"Metric 4 - Orig vs wrong GPT token count match:\", orig_vs_false_gpt, f\"({orig_vs_false_gpt / total_with_ann:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs comparrison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_df = pd.read_csv(\"/Users/linndfors/study/diploma/ner_for_fem/data/LLM_models_comparisson/MAIN_finetuned_aya_for_the_test_parallel_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finetuned</th>\n",
       "      <th>references</th>\n",
       "      <th>inputs</th>\n",
       "      <th>double_finetuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>офіцерка</td>\n",
       "      <td>офіцерка</td>\n",
       "      <td>офіцер</td>\n",
       "      <td>офіцер</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>мільярдер</td>\n",
       "      <td>мільярдер</td>\n",
       "      <td>мільярдерка</td>\n",
       "      <td>мільярдерка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>пакистанець</td>\n",
       "      <td>пакистанець</td>\n",
       "      <td>пакистанка</td>\n",
       "      <td>пакистанка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Статутні внески найбільшого розміру не задекла...</td>\n",
       "      <td>Статутні внески найбільшого розміру не задекла...</td>\n",
       "      <td>Статутні внески найбільшого розміру не задекла...</td>\n",
       "      <td>Статутні внески найбільшого розміру не задекла...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>отоларингологиня</td>\n",
       "      <td>отоларингологиня</td>\n",
       "      <td>отоларинголог</td>\n",
       "      <td>отоларинголог</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>Зазначається, що це спільна власність судді та...</td>\n",
       "      <td>Зазначається, що це спільна власність судді та...</td>\n",
       "      <td>Зазначається, що це спільна власність судді та...</td>\n",
       "      <td>Зазначається, що це спільна власність судді та...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>Львів», які перевірила декларації депутатів ЛМ...</td>\n",
       "      <td>Львів», які перевірила декларації депутатів ЛМ...</td>\n",
       "      <td>Львів», які перевірила декларації депутаток ЛМ...</td>\n",
       "      <td>Львів», які перевірила декларації депутаток ЛМ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>- А ти що - підглядав? - питає міліціонер.</td>\n",
       "      <td>- А ти що - підглядав ? - питає міліціонер .</td>\n",
       "      <td>- А ти що - підглядав ? - питає міліціонерка .\\n</td>\n",
       "      <td>- А ти що - підглядав? - питає міліціонерка.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>Працівник розплідника Брагіна Олександр Черняє...</td>\n",
       "      <td>Працівник розплідника Брагіна Олександр Черняє...</td>\n",
       "      <td>Працівниця розплідника Брагіна Олександра Черн...</td>\n",
       "      <td>Працівниця розплідника Брагіна Валентина Черня...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>кар’єристка</td>\n",
       "      <td>кар’єристка</td>\n",
       "      <td>кар’єрист</td>\n",
       "      <td>кар’єрист</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>523 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             finetuned  \\\n",
       "0                                             офіцерка   \n",
       "1                                            мільярдер   \n",
       "2                                          пакистанець   \n",
       "3    Статутні внески найбільшого розміру не задекла...   \n",
       "4                                     отоларингологиня   \n",
       "..                                                 ...   \n",
       "518  Зазначається, що це спільна власність судді та...   \n",
       "519  Львів», які перевірила декларації депутатів ЛМ...   \n",
       "520         - А ти що - підглядав? - питає міліціонер.   \n",
       "521  Працівник розплідника Брагіна Олександр Черняє...   \n",
       "522                                        кар’єристка   \n",
       "\n",
       "                                            references  \\\n",
       "0                                             офіцерка   \n",
       "1                                            мільярдер   \n",
       "2                                          пакистанець   \n",
       "3    Статутні внески найбільшого розміру не задекла...   \n",
       "4                                     отоларингологиня   \n",
       "..                                                 ...   \n",
       "518  Зазначається, що це спільна власність судді та...   \n",
       "519  Львів», які перевірила декларації депутатів ЛМ...   \n",
       "520       - А ти що - підглядав ? - питає міліціонер .   \n",
       "521  Працівник розплідника Брагіна Олександр Черняє...   \n",
       "522                                        кар’єристка   \n",
       "\n",
       "                                                inputs  \\\n",
       "0                                               офіцер   \n",
       "1                                          мільярдерка   \n",
       "2                                           пакистанка   \n",
       "3    Статутні внески найбільшого розміру не задекла...   \n",
       "4                                        отоларинголог   \n",
       "..                                                 ...   \n",
       "518  Зазначається, що це спільна власність судді та...   \n",
       "519  Львів», які перевірила декларації депутаток ЛМ...   \n",
       "520   - А ти що - підглядав ? - питає міліціонерка .\\n   \n",
       "521  Працівниця розплідника Брагіна Олександра Черн...   \n",
       "522                                          кар’єрист   \n",
       "\n",
       "                                      double_finetuned  \n",
       "0                                               офіцер  \n",
       "1                                          мільярдерка  \n",
       "2                                           пакистанка  \n",
       "3    Статутні внески найбільшого розміру не задекла...  \n",
       "4                                        отоларинголог  \n",
       "..                                                 ...  \n",
       "518  Зазначається, що це спільна власність судді та...  \n",
       "519  Львів», які перевірила декларації депутаток ЛМ...  \n",
       "520       - А ти що - підглядав? - питає міліціонерка.  \n",
       "521  Працівниця розплідника Брагіна Валентина Черня...  \n",
       "522                                          кар’єрист  \n",
       "\n",
       "[523 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "523it [07:44,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "swapped_vs_finetuned_match = 0\n",
    "orig_and_double_finetuned_match = 0\n",
    "eval_total = 0\n",
    "\n",
    "for x, row in tqdm.tqdm(finetuned_df.iterrows()):\n",
    "    swapped = ner_nlp(row['references'])\n",
    "    finetuned = ner_nlp(row['finetuned'])\n",
    "    orig = ner_nlp(row['inputs'])\n",
    "    double_finetuned = ner_nlp(row['double_finetuned'])\n",
    "\n",
    "    if len(swapped) == len(finetuned):\n",
    "        swapped_vs_finetuned_match += 1\n",
    "\n",
    "    if len(orig) == len(double_finetuned):\n",
    "        orig_and_double_finetuned_match += 1\n",
    "\n",
    "    eval_total += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact match for JOB entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_entities(doc):\n",
    "    return list(ent.text for ent in doc.ents if ent.label_ == \"JOB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 523/523 [00:00<00:00, 7635.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "ref_finetuned_exact_match = 0\n",
    "orig_double_finetuned_exact_match = 0\n",
    "total_rows = len(finetuned_df)\n",
    "\n",
    "for _, row in tqdm.tqdm(finetuned_df.iterrows(), total=total_rows):\n",
    "    if row['references'] == row['finetuned']:\n",
    "        ref_finetuned_exact_match += 1\n",
    "    if row['inputs'] == row['double_finetuned']:\n",
    "        orig_double_finetuned_exact_match += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact JOB match between reference and finetuned: 43.98%\n",
      "Exact JOB match between original and double-finetuned: 52.01%\n"
     ]
    }
   ],
   "source": [
    "ref_finetuned_exact_match_pct = (ref_finetuned_exact_match / total_rows) * 100\n",
    "orig_double_finetuned_exact_match_pct = (orig_double_finetuned_exact_match / total_rows) * 100\n",
    "\n",
    "print(f\"Exact JOB match between reference and finetuned: {ref_finetuned_exact_match_pct:.2f}%\")\n",
    "print(f\"Exact JOB match between original and double-finetuned: {orig_double_finetuned_exact_match_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "ref_finetuned_match_count = 0\n",
    "orig_double_finetuned_match_count = 0\n",
    "total_job_ents = 0\n",
    "\n",
    "ref_jobs_c, orig_jobs_c, finetuned_jobs_c, double_finetuned_jobs_c = 0,0,0,0\n",
    "\n",
    "for _, row in tqdm.tqdm(finetuned_df.iterrows(), total=total_rows):\n",
    "    swapped = ner_nlp(row['references'])\n",
    "    finetuned = ner_nlp(row['finetuned'])\n",
    "    orig = ner_nlp(row['inputs'])\n",
    "    double_finetuned = ner_nlp(row['double_finetuned'])\n",
    "\n",
    "    orig_jobs = extract_job_entities(orig)\n",
    "    ref_jobs = extract_job_entities(swapped)\n",
    "    finetuned_jobs = extract_job_entities(finetuned)\n",
    "    double_finetuned_jobs = extract_job_entities(double_finetuned)\n",
    "\n",
    "    ref_jobs_c += len(ref_jobs)\n",
    "    orig_jobs_c += len(orig_jobs)\n",
    "    finetuned_jobs_c += len(finetuned_jobs)\n",
    "    double_finetuned_jobs_c += len(double_finetuned_jobs)\n",
    "\n",
    "\n",
    "    len_of_job_list_swapped = min(len(ref_jobs), len(finetuned_jobs))\n",
    "    \n",
    "    for n in range(len_of_job_list_swapped):\n",
    "        if ref_jobs[n] == finetuned_jobs[n]:\n",
    "            ref_finetuned_match_count += 1\n",
    "\n",
    "    len_of_job_list_orig = min(len(orig_jobs), len(double_finetuned_jobs))\n",
    "\n",
    "    for n in range(len_of_job_list_orig):\n",
    "        if orig_jobs[n] == double_finetuned_jobs[n]:\n",
    "            orig_double_finetuned_match_count += 1\n",
    "\n",
    "# total_job_ents = sum(ref_jobs, orig_jobs, finetuned_jobs, double_finetuned_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact JOB match between reference and finetuned: 82.65%\n",
      "Exact JOB match between original and double-finetuned: 87.92%\n"
     ]
    }
   ],
   "source": [
    "ref_finetuned_match_pct = (ref_finetuned_match_count / ref_jobs_c) * 100\n",
    "orig_double_finetuned_match_pct = (orig_double_finetuned_match_count / orig_jobs_c) * 100\n",
    "\n",
    "print(f\"Exact JOB match between reference and finetuned: {ref_finetuned_match_pct:.2f}%\")\n",
    "print(f\"Exact JOB match between original and double-finetuned: {orig_double_finetuned_match_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 523\n",
      "\n",
      "Metric 1 - Swapped vs Finetuned token count match: 481 (91.97%)\n",
      "Metric 2 - Orig vs Double Finetuned token count match: 487 (93.12%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total sentences:\", eval_total)\n",
    "print(\"\\nMetric 1 - Swapped vs Finetuned token count match:\", swapped_vs_finetuned_match, f\"({swapped_vs_finetuned_match / eval_total:.2%})\")\n",
    "print(\"Metric 2 - Orig vs Double Finetuned token count match:\", orig_and_double_finetuned_match, f\"({orig_and_double_finetuned_match / eval_total:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.read_csv(\"/Users/linndfors/study/diploma/ner_for_fem/data/LLM_models_comparisson/MAIN_original_aya_for_the_test_parallel_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>references</th>\n",
       "      <th>inputs</th>\n",
       "      <th>original</th>\n",
       "      <th>double_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>офіцерка</td>\n",
       "      <td>офіцер</td>\n",
       "      <td>офіцер</td>\n",
       "      <td>офіцер - жінка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>мільярдер</td>\n",
       "      <td>мільярдерка</td>\n",
       "      <td>мільярдер</td>\n",
       "      <td>мільярдерка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>пакистанець</td>\n",
       "      <td>пакистанка</td>\n",
       "      <td>пакистанець</td>\n",
       "      <td>пакистанка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Статутні внески найбільшого розміру не задекла...</td>\n",
       "      <td>Статутні внески найбільшого розміру не задекла...</td>\n",
       "      <td>Статутний внесок найбільшого розміру не задекл...</td>\n",
       "      <td>Статутний внесок найбільшого розміру не задекл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>отоларингологиня</td>\n",
       "      <td>отоларинголог</td>\n",
       "      <td>отоларинголог - жінка</td>\n",
       "      <td>отоларинголог - чоловік</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          references  \\\n",
       "0                                           офіцерка   \n",
       "1                                          мільярдер   \n",
       "2                                        пакистанець   \n",
       "3  Статутні внески найбільшого розміру не задекла...   \n",
       "4                                   отоларингологиня   \n",
       "\n",
       "                                              inputs  \\\n",
       "0                                             офіцер   \n",
       "1                                        мільярдерка   \n",
       "2                                         пакистанка   \n",
       "3  Статутні внески найбільшого розміру не задекла...   \n",
       "4                                      отоларинголог   \n",
       "\n",
       "                                            original  \\\n",
       "0                                             офіцер   \n",
       "1                                          мільярдер   \n",
       "2                                        пакистанець   \n",
       "3  Статутний внесок найбільшого розміру не задекл...   \n",
       "4                              отоларинголог - жінка   \n",
       "\n",
       "                                     double_original  \n",
       "0                                     офіцер - жінка  \n",
       "1                                        мільярдерка  \n",
       "2                                         пакистанка  \n",
       "3  Статутний внесок найбільшого розміру не задекл...  \n",
       "4                            отоларинголог - чоловік  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def edit_ents(raw_output, orig_row):\n",
    "#     if not orig_row.strip().endswith(\".\") and raw_output.endswith(\".\"):\n",
    "#         raw_output = raw_output[:-1]\n",
    "\n",
    "#     if orig_row[0].islower():\n",
    "#         return raw_output[0].lower() + raw_output[1:]\n",
    "#     return raw_output\n",
    "\n",
    "# orig_df['original'] = orig_df.apply(lambda x: edit_ents(x['original'], x['inputs']), axis=1)\n",
    "# orig_df['double_original'] = orig_df.apply(lambda x: edit_ents(x['double_original'], x['original']), axis=1)\n",
    "# orig_df.to_csv(\"/Users/linndfors/study/diploma/aya-finetuned_eval_results(comparisson)/MAIN_original_aya_for_the_test_parallel_dataset_1.csv\",index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swapped_vs_original_match = 0\n",
    "orig_and_double_original_match = 0\n",
    "eval_orig_total = 0\n",
    "\n",
    "for x, row in tqdm.tqdm(orig_df.iterrows()):\n",
    "    swapped = ner_nlp(row['references'])\n",
    "    original = ner_nlp(row['original'])\n",
    "    orig = ner_nlp(row['inputs'])\n",
    "    double_original = ner_nlp(row['double_original'])\n",
    "\n",
    "    if len(swapped) == len(original):\n",
    "        swapped_vs_original_match += 1\n",
    "\n",
    "    if len(orig) == len(double_original):\n",
    "        orig_and_double_original_match += 1\n",
    "\n",
    "    eval_orig_total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 523\n",
      "\n",
      "Metric 1 - Swapped vs Original Aya token count match: 364 (69.60%)\n",
      "Metric 2 - Orig vs Double Original Aya token count match: 336 (64.24%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total sentences:\", eval_orig_total)\n",
    "print(\"\\nMetric 1 - Swapped vs Original Aya token count match:\", swapped_vs_original_match, f\"({swapped_vs_original_match / eval_orig_total:.2%})\")\n",
    "print(\"Metric 2 - Orig vs Double Original Aya token count match:\", orig_and_double_original_match, f\"({orig_and_double_original_match / eval_orig_total:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "523it [06:56,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "ref_orig_aya_match_count = 0\n",
    "orig_double_orig_aya_match_count = 0\n",
    "total_job_ents = 0\n",
    "\n",
    "ref_jobs_c, orig_jobs_c, original_jobs_c, double_original_jobs_c = 0,0,0,0\n",
    "\n",
    "for _, row in tqdm.tqdm(orig_df.iterrows()):\n",
    "    swapped = ner_nlp(row['references'])\n",
    "    original = ner_nlp(row['original'])\n",
    "    orig = ner_nlp(row['inputs'])\n",
    "    double_original = ner_nlp(row['double_original'])\n",
    "\n",
    "    orig_jobs = extract_job_entities(orig)\n",
    "    ref_jobs = extract_job_entities(swapped)\n",
    "    original_jobs = extract_job_entities(original)\n",
    "    double_original_jobs = extract_job_entities(double_original)\n",
    "\n",
    "    ref_jobs_c += len(ref_jobs)\n",
    "    orig_jobs_c += len(orig_jobs)\n",
    "    original_jobs_c += len(original_jobs)\n",
    "    double_original_jobs_c += len(double_original_jobs)\n",
    "\n",
    "    len_of_job_list_swapped = min(len(ref_jobs), len(original_jobs))\n",
    "    \n",
    "    for n in range(len_of_job_list_swapped):\n",
    "        if ref_jobs[n] == original_jobs[n]:\n",
    "            ref_orig_aya_match_count += 1\n",
    "        # else:\n",
    "        #     print(ref_jobs[n], \"-\", original_jobs[n])\n",
    "\n",
    "    len_of_job_list_orig = min(len(orig_jobs), len(double_original_jobs))\n",
    "\n",
    "    for n in range(len_of_job_list_orig):\n",
    "        if orig_jobs[n] == double_original_jobs[n]:\n",
    "            orig_double_orig_aya_match_count += 1\n",
    "        # else:\n",
    "        #     print(orig_jobs[n], \"-\", double_original_jobs[n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact JOB match between reference and orig Aya: 30.36%\n",
      "Exact JOB match between original and double-orig Aya: 76.09%\n"
     ]
    }
   ],
   "source": [
    "ref_original_match_pct = (ref_orig_aya_match_count / ref_jobs_c) * 100\n",
    "orig_double_original_match_pct = (orig_double_orig_aya_match_count / orig_jobs_c) * 100\n",
    "\n",
    "print(f\"Exact JOB match between reference and orig Aya: {ref_original_match_pct:.2f}%\")\n",
    "print(f\"Exact JOB match between original and double-orig Aya: {orig_double_original_match_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_df = pd.read_csv(\"/Users/linndfors/study/diploma/ner_for_fem/data/LLM_models_comparisson/MAIN_gpt_for_the_test_parallel_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>references</th>\n",
       "      <th>inputs</th>\n",
       "      <th>gpt</th>\n",
       "      <th>double_gpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>офіцерка</td>\n",
       "      <td>офіцер</td>\n",
       "      <td>офіцерка</td>\n",
       "      <td>офіцер</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>мільярдер</td>\n",
       "      <td>мільярдерка</td>\n",
       "      <td>мільярдер</td>\n",
       "      <td>мільярдерка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>пакистанець</td>\n",
       "      <td>пакистанка</td>\n",
       "      <td>пакистанець</td>\n",
       "      <td>пакистанка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Статутні внески найбільшого розміру не задекла...</td>\n",
       "      <td>Статутні внески найбільшого розміру не задекла...</td>\n",
       "      <td>Статутні внески найбільшого розміру не задекла...</td>\n",
       "      <td>Статутні внески найбільшого розміру не задекла...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>отоларингологиня</td>\n",
       "      <td>отоларинголог</td>\n",
       "      <td>отоларингологиня</td>\n",
       "      <td>отоларинголог</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          references  \\\n",
       "0                                           офіцерка   \n",
       "1                                          мільярдер   \n",
       "2                                        пакистанець   \n",
       "3  Статутні внески найбільшого розміру не задекла...   \n",
       "4                                   отоларингологиня   \n",
       "\n",
       "                                              inputs  \\\n",
       "0                                             офіцер   \n",
       "1                                        мільярдерка   \n",
       "2                                         пакистанка   \n",
       "3  Статутні внески найбільшого розміру не задекла...   \n",
       "4                                      отоларинголог   \n",
       "\n",
       "                                                 gpt  \\\n",
       "0                                           офіцерка   \n",
       "1                                          мільярдер   \n",
       "2                                        пакистанець   \n",
       "3  Статутні внески найбільшого розміру не задекла...   \n",
       "4                                   отоларингологиня   \n",
       "\n",
       "                                          double_gpt  \n",
       "0                                             офіцер  \n",
       "1                                        мільярдерка  \n",
       "2                                         пакистанка  \n",
       "3  Статутні внески найбільшого розміру не задекла...  \n",
       "4                                      отоларинголог  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "523it [08:08,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "swapped_vs_gpt_match = 0\n",
    "orig_and_double_gpt_match = 0\n",
    "eval_gpt_total = 0\n",
    "\n",
    "for x, row in tqdm.tqdm(gpt_df.iterrows()):\n",
    "    swapped = ner_nlp(row['references'])\n",
    "    gpt = ner_nlp(row['gpt'])\n",
    "    orig = ner_nlp(row['inputs'])\n",
    "    double_gpt = ner_nlp(row['double_gpt'])\n",
    "\n",
    "    if len(swapped) == len(gpt):\n",
    "        swapped_vs_gpt_match += 1\n",
    "\n",
    "    if len(orig) == len(double_gpt):\n",
    "        orig_and_double_gpt_match += 1\n",
    "\n",
    "    eval_gpt_total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 523\n",
      "\n",
      "Metric 1 - Swapped vs GPT token count match: 475 (90.82%)\n",
      "Metric 2 - Orig vs Double GPT token count match: 475 (90.82%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total sentences:\", eval_gpt_total)\n",
    "print(\"\\nMetric 1 - Swapped vs GPT token count match:\", swapped_vs_gpt_match, f\"({swapped_vs_gpt_match / eval_gpt_total:.2%})\")\n",
    "print(\"Metric 2 - Orig vs Double GPT token count match:\", orig_and_double_gpt_match, f\"({orig_and_double_gpt_match / eval_gpt_total:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_gpt_match_count = 0\n",
    "orig_double_gpt_match_count = 0\n",
    "total_job_ents = 0\n",
    "\n",
    "ref_jobs_c, orig_jobs_c, gpt_jobs_c, double_gpt_jobs_c = 0,0,0,0\n",
    "\n",
    "for _, row in tqdm.tqdm(gpt_df.iterrows()):\n",
    "    swapped = ner_nlp(row['references'])\n",
    "    gpt = ner_nlp(row['gpt'])\n",
    "    orig = ner_nlp(row['inputs'])\n",
    "    double_gpt = ner_nlp(row['double_gpt'])\n",
    "\n",
    "    orig_jobs = extract_job_entities(orig)\n",
    "    ref_jobs = extract_job_entities(swapped)\n",
    "    gpt_jobs = extract_job_entities(gpt)\n",
    "    double_gpt_jobs = extract_job_entities(double_gpt)\n",
    "\n",
    "    ref_jobs_c += len(ref_jobs)\n",
    "    orig_jobs_c += len(orig_jobs)\n",
    "    gpt_jobs_c += len(gpt_jobs)\n",
    "    double_gpt_jobs_c += len(double_gpt_jobs)\n",
    "\n",
    "    len_of_job_list_swapped = min(len(ref_jobs), len(gpt_jobs))\n",
    "    \n",
    "    for n in range(len_of_job_list_swapped):\n",
    "        if ref_jobs[n] == gpt_jobs[n]:\n",
    "            ref_gpt_match_count += 1\n",
    "        else:\n",
    "            print(ref_jobs[n], \"-\", gpt_jobs[n])\n",
    "\n",
    "    len_of_job_list_orig = min(len(orig_jobs), len(double_gpt_jobs))\n",
    "\n",
    "    for n in range(len_of_job_list_orig):\n",
    "        if orig_jobs[n] == double_gpt_jobs[n]:\n",
    "            orig_double_gpt_match_count += 1\n",
    "        else:\n",
    "            print(orig_jobs[n], \"-\", double_gpt_jobs[n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_gpt_match_pct = (ref_gpt_match_count / total_rows) * 100\n",
    "orig_double_gpt_match_pct = (orig_double_gpt_match_count / total_rows) * 100\n",
    "\n",
    "print(f\"Exact JOB match between reference and gpt: {ref_gpt_match_pct:.2f}%\")\n",
    "print(f\"Exact JOB match between original and double-gpt: {orig_double_gpt_match_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove PERS entities and check exact match metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def remove_pers_entities(doc):\n",
    "    \"\"\"Remove all PERS entities from the text based on their spans.\"\"\"\n",
    "    tokens = [token.text for token in doc if (token.ent_type_ != \"PERS\" and token.text != \"\\n\")]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "match_count_ref_finetuned = 0\n",
    "match_count_orig_double = 0\n",
    "\n",
    "total_pairs = 0\n",
    "\n",
    "for _, row in tqdm.tqdm(finetuned_df.iterrows()):\n",
    "    doc_ref = ner_nlp(row['references'])\n",
    "    doc_finetuned = ner_nlp(row['finetuned'])\n",
    "    doc_orig = ner_nlp(row['inputs'])\n",
    "    doc_double = ner_nlp(row['double_finetuned'])\n",
    "\n",
    "    ref_clean = remove_pers_entities(doc_ref)\n",
    "    finetuned_clean = remove_pers_entities(doc_finetuned)\n",
    "    orig_clean = remove_pers_entities(doc_orig)\n",
    "    double_clean = remove_pers_entities(doc_double)\n",
    "\n",
    "    total_pairs += 1\n",
    "\n",
    "    if ref_clean == finetuned_clean:\n",
    "        match_count_ref_finetuned += 1\n",
    "    else:\n",
    "        print(ref_clean, \"--\", row['references'])\n",
    "        print(finetuned_clean, \"--\", row['finetuned'])\n",
    "        print( \"----\\n\")\n",
    "        print(f\"[REF ≠ FINETUNED]\\n{ref_clean}\\n{finetuned_clean}\\n\")\n",
    "\n",
    "    if orig_clean == double_clean:\n",
    "        match_count_orig_double += 1\n",
    "    else:\n",
    "        print(f\"[ORIG ≠ DOUBLE-FINETUNED]\\n{orig_clean}\\n{double_clean}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STATISTICS]\n",
      "Exact matches (REF vs FINETUNED): 0.6424474187380497\n",
      "Exact matches (ORIG vs DOUBLE-FINETUNED): 0.739961759082218\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n[STATISTICS]\")\n",
    "print(f\"Exact matches (REF vs FINETUNED): {match_count_ref_finetuned/total_pairs}\")\n",
    "print(f\"Exact matches (ORIG vs DOUBLE-FINETUNED): {match_count_orig_double/total_pairs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_count_ref_gpt = 0\n",
    "match_count_orig_double_gpt = 0\n",
    "total_pairs = 0\n",
    "\n",
    "for _, row in tqdm.tqdm(gpt_df.iterrows()):\n",
    "    swapped = ner_nlp(row['references'])\n",
    "    gpt = ner_nlp(row['gpt'])\n",
    "    orig = ner_nlp(row['inputs'])\n",
    "    double_gpt = ner_nlp(row['double_gpt'])\n",
    "\n",
    "    ref_clean = remove_pers_entities(swapped)\n",
    "    gpt_clean = remove_pers_entities(gpt)\n",
    "    orig_clean = remove_pers_entities(orig)\n",
    "    double_gpt_clean = remove_pers_entities(double_gpt)\n",
    "\n",
    "    total_pairs += 1\n",
    "\n",
    "    if ref_clean == gpt_clean:\n",
    "        match_count_ref_gpt += 1\n",
    "    else:\n",
    "        print(f\"[REF ≠ FINETUNED]\\n{ref_clean}\\n{gpt_clean}\\n\")\n",
    "\n",
    "    if orig_clean == double_gpt_clean:\n",
    "        match_count_orig_double_gpt += 1\n",
    "    else:\n",
    "        print(f\"[ORIG ≠ DOUBLE-FINETUNED]\\n{orig_clean}\\n{double_gpt_clean}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STATISTICS]\n",
      "Exact matches (REF vs GPT): 0.6233269598470363\n",
      "Exact matches (ORIG vs DOUBLE-GPT): 0.7017208413001912\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n[STATISTICS]\")\n",
    "print(f\"Exact matches (REF vs GPT): {match_count_ref_gpt/total_pairs}\")\n",
    "print(f\"Exact matches (ORIG vs DOUBLE-GPT): {match_count_orig_double_gpt/total_pairs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "523it [07:25,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "match_count_ref_original = 0\n",
    "match_count_orig_double_original = 0\n",
    "total_pairs = 0\n",
    "\n",
    "for _, row in tqdm.tqdm(orig_df.iterrows()):\n",
    "    swapped = ner_nlp(row['references'])\n",
    "    original = ner_nlp(row['original'])\n",
    "    orig = ner_nlp(row['inputs'])\n",
    "    double_original = ner_nlp(row['double_original'])\n",
    "\n",
    "    ref_clean = remove_pers_entities(swapped)\n",
    "    original_clean = remove_pers_entities(original)\n",
    "    orig_clean = remove_pers_entities(orig)\n",
    "    double_original_clean = remove_pers_entities(double_original)\n",
    "\n",
    "    total_pairs += 1\n",
    "\n",
    "    if ref_clean == original_clean:\n",
    "        match_count_ref_original += 1\n",
    "    else:\n",
    "        pass\n",
    "        # print(f\"[REF ≠ FINETUNED]\\n{ref_clean}\\n{original_clean}\\n\")\n",
    "\n",
    "    if orig_clean == double_original_clean:\n",
    "        match_count_orig_double_original += 1\n",
    "    else:\n",
    "        pass\n",
    "        # print(f\"[ORIG ≠ DOUBLE-FINETUNED]\\n{orig_clean}\\n{double_original_clean}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STATISTICS]\n",
      "Exact matches (REF vs Original Aya-101): 0.22562141491395793\n",
      "Exact matches (ORIG vs DOUBLE-Aya-101): 0.3403441682600382\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n[STATISTICS]\")\n",
    "print(f\"Exact matches (REF vs Original Aya-101): {match_count_ref_original/total_pairs}\")\n",
    "print(f\"Exact matches (ORIG vs DOUBLE-Aya-101): {match_count_orig_double_original/total_pairs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary terms metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bruk_pairs = pd.read_csv(\"/Users/linndfors/study/diploma/ner_for_fem/data/util/dataset_with_filename_and_pair_gender-swapped_words/bruk_gender_pairs_from_swapping_by_gender.csv\", index_col = 0)\n",
    "ng_pairs = pd.read_csv(\"/Users/linndfors/study/diploma/ner_for_fem/data/util/dataset_with_filename_and_pair_gender-swapped_words/ng_gender_pairs_from_swapping_by_gender.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_pers_dict = pd.read_csv(\"/Users/linndfors/study/diploma/uk-gender-word-mapper/gender_pairs_dictionary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>абітурієнт</td>\n",
       "      <td>абітурієнтка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>абонент</td>\n",
       "      <td>абонентка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>абориген</td>\n",
       "      <td>аборигенка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>абстракціоніст</td>\n",
       "      <td>абстракціоністка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>авантюрист</td>\n",
       "      <td>авантюристка</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             male            female\n",
       "0      абітурієнт      абітурієнтка\n",
       "1         абонент         абонентка\n",
       "2        абориген        аборигенка\n",
       "3  абстракціоніст  абстракціоністка\n",
       "4      авантюрист      авантюристка"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_pers_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/linndfors/study/diploma/uk-gender-word-mapper/common_gender_words_list.txt') as file:\n",
    "    common_gender_words_list = [line.strip() for line in file if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict values: 722\n",
      "Not dict values: 83\n",
      "JOB for NG: dict metric: 0.8968944099378882\n"
     ]
    }
   ],
   "source": [
    "in_dict = 0\n",
    "not_in_dict = 0\n",
    "not_in_dict_list = []\n",
    "\n",
    "for x in ng_pairs['original']:\n",
    "    if x in gender_pers_dict['male'].values or x in gender_pers_dict['female'].values or x in common_gender_words_list:\n",
    "        in_dict += 1\n",
    "    else:\n",
    "        not_in_dict += 1\n",
    "        not_in_dict_list.append(x)\n",
    "\n",
    "print(f\"Dict values: {in_dict}\")\n",
    "print(f\"Not dict values: {not_in_dict}\")\n",
    "print(f\"JOB for NG: dict metric: {in_dict/len(ng_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict values: 263\n",
      "Not dict values: 56\n",
      "JOB for BRUK: dict metric: 0.8244514106583072\n"
     ]
    }
   ],
   "source": [
    "in_dict = 0\n",
    "not_in_dict = 0\n",
    "not_in_dict_list = []\n",
    "\n",
    "for x in bruk_pairs['original']:\n",
    "    if x in gender_pers_dict['male'].values or x in gender_pers_dict['female'].values or x in common_gender_words_list:\n",
    "        in_dict += 1\n",
    "    else:\n",
    "        not_in_dict += 1\n",
    "        not_in_dict_list.append(x)\n",
    "    \n",
    "print(f\"Dict values: {in_dict}\")\n",
    "print(f\"Not dict values: {not_in_dict}\")\n",
    "print(f\"JOB for BRUK: dict metric: {in_dict/len(bruk_pairs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
